---
title: Agent 的进阶应用
author: 凌杰
date: 2026-02-19
tags: Prompt MCP Skills
categories: 人工智能
---

> [!NOTE] 笔记说明
>
> 这篇笔记是《[[Agent 的基础应用]]》一文的后续内容。其中记录了我学习 AI Agent 的扩展机制，并将其应用于实际工作场景的全过程，以及在该过程中所获得的心得体会。同样的，这些内容也将成为我 AI 系列笔记的一部分，被存储在本人 Github 上的[计算机学习笔记库](https://github.com/owlman/CS_StudyNotes)中，并予以长期维护。

## AI Agent 的扩展机制

在《[[Agent 的基础应用]]》一文中，我对于 Agent 的应用演示都是基于简单的提示词（Prompt）来进行的。但在实际生产环境中，我们要描述的问题会远比这些演示复杂得多，这不仅需要掌握更专业的提示词写法，而且还需要充分利用 Agent 提供的各种扩展机制来提高提示词的命中率。现在，让我们从提示词本身及其能力边界来开始介绍，逐步展开接下来的进阶之旅。

### 提示词及其能力边界

在将 Agent 工具具体应用到实际的生产环境中之前，人们首先需要弄清楚的是：提示词在这类应用中的作用到底是什么？它的能力边界在哪里？如果我们在这两个问题上的理解出现了偏差，那么后续的一切扩展机制都会被错误地理解为一种更高级的提示词技巧。

考虑到 LLM 的核心训练机制是在高维参数空间中寻找一个在给定数据分布上表现足够好的函数近似，它的具体推理过程永远都是在根据某一概率分布来输出下一个文本单元（在专业术语中，我们称之为“Token”，关于这个单位的具体计算方法，读者可参考我稍后在“参考资料”一节中所提供的视频教程：《关于 Token 的科普》）。换言之，这不是一个基于显式规则或程序控制流的命令执行系统。因此从技术本质上看，当我们向 LLM 提供一个提示词时，并不是在下“命令”或写“代码”，而是在向它注入符合当前环境需求的上下文信息，从而改变其输出的概率分布。这就意味着：

> 提示词调整的是 LLM 输出的概率倾向，它无法改变 LLM 的能力边界。

举个例子，当我们在 Agent 应用中输入并提交如下语句作为提示词时，它们的功能分别是：

- “你是一名专业的法律顾问”：用于角色塑造（Persona），影响语气、知识调用倾向与表达方式。
- “请以 JSON 格式输出”：用于输出约束（Format Constraints），规范结果结构，提高可读性与可解析性。
- “请分步骤推理”：用于任务定义（Task Framing），明确问题范围，限制推理方向。

由此可见，提示词的作用本质上都是在向 LLM 注入用于影响其行为模式的额外上下文信息。我们使用提示词的技巧或许可以显著提高任务完成质量。但这些终归只是一种“软控制”的手段。其有效性取决于 LLM 的内部能力，而非系统层面的强约束。它具有三个典型特征。

1. **不确定性**：相同的提示词，在不同时间、不同上下文长度、甚至不同模型版本下，都可能产生差异结果。提示词并不能保证稳定行为。
2. **非隔离性**：多轮对话中的历史信息可能影响当前输出；规则之间也可能互相干扰。提示词并不具备真正的“作用域隔离”。
3. **不可验证性**：提示词很难像代码一样做单元测试。一次微小改动，可能影响多个场景；而这种影响往往难以预判。

因此，当问题涉及到 Agent 应用的“能力扩展”时，我们需要做的就不再仅仅是“写好提示词”那么简单了。因为提示词虽然可以很好地引导 LLM 的推理路径、规范其输出格式并优化文本的表达质量，从而在一定程度上提升任务的成功率，但对于面向生产环境的具体应用，以下能力是提示词无法提供的：

- **调用外部的服务/API**：因为这需要独立于 LLM 之外的代码执行环境，以及相关的程序逻辑支持；
- **管理应用的执行状态**：因为这需要执行面向数据库管理系统、文件管理系统的增删改查操作；
- **保证行为逻辑的可复用**：因为即使是相同的提示词，它在不同时空条件下会产生不同的结果；

除了能力层面的限制，提示词还存在工程与经济层面的约束，它所带来的 LLM 计算成本也会给 Agent 应用带来另一种意义上的能力边界。众所周知，如今的主流 LLM 服务提供商（例如 OpenAI、Anthropic 等）都是以 Token 为单位来计费的。用户与 LLM 的每次对话，都会产生一定数量的 Token 消耗，越复杂的对话消耗的 Token 数量就越多。因此，当我们在对话中叠加越来越多的提示词时，免不了会导致系统成本的大幅上升。在个人使用场景中，这个成本或许还尚可承受，一旦进入到具体的生产环境中，问题就会迅速被放大，它主要体现在以下四个方面。

1. **版本管理困难**：提示词通常以自然语言形式存在，缺乏清晰的版本结构。修改后很难精确追踪行为变化；
2. **行为回归问题**：一次看似微小的改动，可能导致多个下游场景输出变化，而这些变化难以预估；
3. **可读性下降**：当规则不断叠加时，提示词会逐渐演变成“规则堆砌文本”。新成员难以理解设计意图；
4. **知识隐性化**：大量设计经验隐藏在自然语言中，无法结构化复用，也无法模块化组合。

这意味着：如果我们需要在一个生产系统需要高频调用 LLM，那么一个“臃肿的系统提示词”会成为长期成本负担，这会大大限制我们扩展 Agent 能力的空间。也正是在这样的背景下，才会出现对更高层扩展方式的探索，例如由 Anthropic 提出的 MCP 服务与 Agent Skills 机制。这些都是他们在“如何系统性地扩展能力”这个问题上的探索成果。而这，正是我们接下来要讨论的内容。

### MCP 服务

#### MCP 解决了什么问题？

- 工具接入标准化
- 跨平台复用
- 安全边界管理

#### MCP 与传统 function calling 的区别

可以对比：

OpenAI 的 function calling 机制

讨论：

- 协议 vs API 绑定
- 可移植性
- 开放性

#### MCP 的成本与风险

- 部署复杂度
- 安全问题
- 依赖管理

### Agent Skills

#### Skills 的核心思想

- 将“行为模式”固化
- 可版本化
- 可复用
- 可组合

#### Skills ≠ Prompt 模板

- 技术差异
- 工程差异
- 维护成本差异

#### Skills 的使用边界

- 何时适合封装？
- 何时不该封装？

### 扩展机制的三层决策结构

### 扩展风险与系统复杂度管理

- Skills 爆炸问题
- MCP 服务膨胀问题
- 过度工程化
- 依赖外部服务的风险

## 项目实践演示

### 项目1：个人网站的重构

### 项目2：文档的格式转换

### 项目3：笔记系统的建构

## 结束语

不要只总结“学到了什么”。

总结：

- 能力扩展的层级模型
- 架构决策原则
- 未来可能的演化方向

## 参考资料

- 官方文档：
  - [MCP 简介](https://www.anthropic.com/news/model-context-protocol)
  - [Agent skills 构建指南](https://resources.anthropic.com/hubfs/The-Complete-Guide-to-Building-Skill-for-Claude.pdf)

- 视频教程
  - 关于 Token 的科普：[YouTube 链接](https://www.youtube.com/watch?v=QNiaoD5RxPA) / [Bilibili 链接](https://www.bilibili.com/video/BV1S5miBvEsu)
  - MCP 教程（基础篇）： [YouTube 链接](https://www.youtube.com/watch?v=yjBUnbRgiNs) / [Bilibili 链接](https://www.bilibili.com/video/BV1uronYREWR/)
  - MCP 教程（进阶篇）：[YouTube 链接](https://www.youtube.com/watch?v=zrs_HWkZS5w) / [Bilibili 链接](https://www.bilibili.com/video/BV1Y854zmEg9)
  - Agent Skills 教程：[YouTube 链接](https://www.youtube.com/watch?v=yDc0_8emz7M) / [Bilibili 链接](https://www.bilibili.com/video/BV1cGigBQE6n)

[^1]:AI Agent Skills：2025年10月16日由Anthropic正式推出，同年12月18日将其发布为开放标准。
